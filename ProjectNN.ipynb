{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdf757f-4eb6-4fb2-97a1-0f90c4485677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa72b1e1-6714-490c-bca6-3cfb40674c78",
   "metadata": {},
   "source": [
    "Neural Network Classifier\n",
    "\n",
    "This model is a feed forward Neural Network Classifier based on the University of Wisconsin's breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e098100c-b161-42fc-9338-ecc7fd9a61b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jahna\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (75.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jahna\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jahna\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jahna\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9d396ef-c258-49a3-9259-620c04afb077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75c4129a-dcdc-4d16-bcb8-07700bd79b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None  # print all columns\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea9aaf-786b-47ee-84c2-ed78bc362aee",
   "metadata": {},
   "source": [
    "1. Import data + dataprep\n",
    "The purpose of this dataset is to classify breast cancer cases between malignant (M), and benign (B). Therefore, my classification network will have two output nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2027beb1-1b97-4cda-bef5-90d522823907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from the UCI ML repository\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", \n",
    "                 sep=\",\", header=None)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3570ad1a-9c77-485e-a732-ecc537306f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9       10       11      12      13     14      15        16       17  \\\n",
       "0  0.14710  0.2419  0.07871  1.0950  0.9053  8.589  153.40  0.006399  0.04904   \n",
       "1  0.07017  0.1812  0.05667  0.5435  0.7339  3.398   74.08  0.005225  0.01308   \n",
       "2  0.12790  0.2069  0.05999  0.7456  0.7869  4.585   94.03  0.006150  0.04006   \n",
       "3  0.10520  0.2597  0.09744  0.4956  1.1560  3.445   27.23  0.009110  0.07458   \n",
       "4  0.10430  0.1809  0.05883  0.7572  0.7813  5.438   94.44  0.011490  0.02461   \n",
       "\n",
       "        18       19       20        21     22     23      24      25      26  \\\n",
       "0  0.05373  0.01587  0.03003  0.006193  25.38  17.33  184.60  2019.0  0.1622   \n",
       "1  0.01860  0.01340  0.01389  0.003532  24.99  23.41  158.80  1956.0  0.1238   \n",
       "2  0.03832  0.02058  0.02250  0.004571  23.57  25.53  152.50  1709.0  0.1444   \n",
       "3  0.05661  0.01867  0.05963  0.009208  14.91  26.50   98.87   567.7  0.2098   \n",
       "4  0.05688  0.01885  0.01756  0.005115  22.54  16.67  152.20  1575.0  0.1374   \n",
       "\n",
       "       27      28      29      30       31  \n",
       "0  0.6656  0.7119  0.2654  0.4601  0.11890  \n",
       "1  0.1866  0.2416  0.1860  0.2750  0.08902  \n",
       "2  0.4245  0.4504  0.2430  0.3613  0.08758  \n",
       "3  0.8663  0.6869  0.2575  0.6638  0.17300  \n",
       "4  0.2050  0.4000  0.1625  0.2364  0.07678  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b34ab418-1128-4ff5-a2e0-18faa057aa43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>4.885000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0           2           3           4            5   \\\n",
       "count  5.690000e+02  569.000000  569.000000  569.000000   569.000000   \n",
       "mean   3.037183e+07   14.127292   19.289649   91.969033   654.889104   \n",
       "std    1.250206e+08    3.524049    4.301036   24.298981   351.914129   \n",
       "min    8.670000e+03    6.981000    9.710000   43.790000   143.500000   \n",
       "25%    8.692180e+05   11.700000   16.170000   75.170000   420.300000   \n",
       "50%    9.060240e+05   13.370000   18.840000   86.240000   551.100000   \n",
       "75%    8.813129e+06   15.780000   21.800000  104.100000   782.700000   \n",
       "max    9.113205e+08   28.110000   39.280000  188.500000  2501.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
       "mean     0.096360    0.104341    0.088799    0.048919    0.181162    0.062798   \n",
       "std      0.014064    0.052813    0.079720    0.038803    0.027414    0.007060   \n",
       "min      0.052630    0.019380    0.000000    0.000000    0.106000    0.049960   \n",
       "25%      0.086370    0.064920    0.029560    0.020310    0.161900    0.057700   \n",
       "50%      0.095870    0.092630    0.061540    0.033500    0.179200    0.061540   \n",
       "75%      0.105300    0.130400    0.130700    0.074000    0.195700    0.066120   \n",
       "max      0.163400    0.345400    0.426800    0.201200    0.304000    0.097440   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
       "mean     0.405172    1.216853    2.866059   40.337079    0.007041    0.025478   \n",
       "std      0.277313    0.551648    2.021855   45.491006    0.003003    0.017908   \n",
       "min      0.111500    0.360200    0.757000    6.802000    0.001713    0.002252   \n",
       "25%      0.232400    0.833900    1.606000   17.850000    0.005169    0.013080   \n",
       "50%      0.324200    1.108000    2.287000   24.530000    0.006380    0.020450   \n",
       "75%      0.478900    1.474000    3.357000   45.190000    0.008146    0.032450   \n",
       "max      2.873000    4.885000   21.980000  542.200000    0.031130    0.135400   \n",
       "\n",
       "               18          19          20          21          22          23  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000  569.000000  569.000000   \n",
       "mean     0.031894    0.011796    0.020542    0.003795   16.269190   25.677223   \n",
       "std      0.030186    0.006170    0.008266    0.002646    4.833242    6.146258   \n",
       "min      0.000000    0.000000    0.007882    0.000895    7.930000   12.020000   \n",
       "25%      0.015090    0.007638    0.015160    0.002248   13.010000   21.080000   \n",
       "50%      0.025890    0.010930    0.018730    0.003187   14.970000   25.410000   \n",
       "75%      0.042050    0.014710    0.023480    0.004558   18.790000   29.720000   \n",
       "max      0.396000    0.052790    0.078950    0.029840   36.040000   49.540000   \n",
       "\n",
       "               24           25          26          27          28  \\\n",
       "count  569.000000   569.000000  569.000000  569.000000  569.000000   \n",
       "mean   107.261213   880.583128    0.132369    0.254265    0.272188   \n",
       "std     33.602542   569.356993    0.022832    0.157336    0.208624   \n",
       "min     50.410000   185.200000    0.071170    0.027290    0.000000   \n",
       "25%     84.110000   515.300000    0.116600    0.147200    0.114500   \n",
       "50%     97.660000   686.500000    0.131300    0.211900    0.226700   \n",
       "75%    125.400000  1084.000000    0.146000    0.339100    0.382900   \n",
       "max    251.200000  4254.000000    0.222600    1.058000    1.252000   \n",
       "\n",
       "               29          30          31  \n",
       "count  569.000000  569.000000  569.000000  \n",
       "mean     0.114606    0.290076    0.083946  \n",
       "std      0.065732    0.061867    0.018061  \n",
       "min      0.000000    0.156500    0.055040  \n",
       "25%      0.064930    0.250400    0.071460  \n",
       "50%      0.099930    0.282200    0.080040  \n",
       "75%      0.161400    0.317900    0.092080  \n",
       "max      0.291000    0.663800    0.207500  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095b1c8-a587-42d9-81b3-251e37243f54",
   "metadata": {},
   "source": [
    "Explore the target variable\n",
    "Let's explore the dependent variable. Its a string column, referring to the 'M' and 'B' targt categories. Before feeding the data in a Neural Network, it will require one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b38d720c-4f83-4c6c-883e-84d44309cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M', 'B'}\n",
      "no. classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(set(df[df.columns[1]]))\n",
    "print('no. classes: ' + str(len(set(df[df.columns[1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a88afbf-60eb-4d14-b3ed-5ea932112dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "classification = pd.get_dummies(df[df.columns[1]])\n",
    "print(classification.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e99bc9-beac-4bb7-b7c8-226417d6d7cb",
   "metadata": {},
   "source": [
    "I obtained a target dataframe, called classification, that contains the one-hot encoded version of my dependent binary variable. Now I can isolate the explanatory variables in my dataframe df. In order to do that I'll drop columns 0 and 1: the first is an index, the second is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "182081aa-93e6-4e0c-826d-86fda296ad69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      2      3       4       5        6        7       8        9       10  \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        11      12      13     14      15        16       17       18  \\\n",
       "0  0.07871  1.0950  0.9053  8.589  153.40  0.006399  0.04904  0.05373   \n",
       "1  0.05667  0.5435  0.7339  3.398   74.08  0.005225  0.01308  0.01860   \n",
       "2  0.05999  0.7456  0.7869  4.585   94.03  0.006150  0.04006  0.03832   \n",
       "3  0.09744  0.4956  1.1560  3.445   27.23  0.009110  0.07458  0.05661   \n",
       "4  0.05883  0.7572  0.7813  5.438   94.44  0.011490  0.02461  0.05688   \n",
       "\n",
       "        19       20        21     22     23      24      25      26      27  \\\n",
       "0  0.01587  0.03003  0.006193  25.38  17.33  184.60  2019.0  0.1622  0.6656   \n",
       "1  0.01340  0.01389  0.003532  24.99  23.41  158.80  1956.0  0.1238  0.1866   \n",
       "2  0.02058  0.02250  0.004571  23.57  25.53  152.50  1709.0  0.1444  0.4245   \n",
       "3  0.01867  0.05963  0.009208  14.91  26.50   98.87   567.7  0.2098  0.8663   \n",
       "4  0.01885  0.01756  0.005115  22.54  16.67  152.20  1575.0  0.1374  0.2050   \n",
       "\n",
       "       28      29      30       31  \n",
       "0  0.7119  0.2654  0.4601  0.11890  \n",
       "1  0.2416  0.1860  0.2750  0.08902  \n",
       "2  0.4504  0.2430  0.3613  0.08758  \n",
       "3  0.6869  0.2575  0.6638  0.17300  \n",
       "4  0.4000  0.1625  0.2364  0.07678  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop([df.columns[0], df.columns[1]], axis=1)   # drop the target variable from df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "393c11b5-3c34-440a-9133-ee5c2fa13d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the shape is:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f7295-b20d-4f18-b6de-2b632d58f534",
   "metadata": {},
   "source": [
    "Before training the model, I turn both explanatory and target data into numpy objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f807829e-f197-47c4-ade6-78f5354d86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.values\n",
    "classification = classification.values\n",
    "# in this case, I uniform the datatypes to float64\n",
    "classification = classification.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3616cb2-e51b-45cb-bb16-862af143710f",
   "metadata": {},
   "source": [
    "\n",
    "Train-Test split\n",
    "In an actual ML job, you would split your dataset in Train, Validation and Testsets. However, this is just an example on how to implement and run a Neural Network, so I'll skip that part and will split the data in train and test only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e806c08-1704-4fd0-a1b3-730969b06944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (426, 30)\n",
      "y_train shape: (426, 2)\n",
      "\n",
      "X_test shape: (143, 30)\n",
      "y_test shape: (143, 2)\n"
     ]
    }
   ],
   "source": [
    "## TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, classification, test_size=0.25, random_state=173)\n",
    "\n",
    "print('X_train shape: ' + str(X_train.shape) + '\\ny_train shape: ' + str(y_train.shape))\n",
    "print('\\nX_test shape: ' + str(X_test.shape) + '\\ny_test shape: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7f9a4-b7df-45bc-9c70-13a314c8ea09",
   "metadata": {},
   "source": [
    "caling the variables must happen after the train-test split. That is because the test set must be scaled using the parameters of the training set: in real world cases you don't know what data you'll get from training, therefore this is the only way to truly understand the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3c9c659-f057-40f5-a7ac-1c5feb86d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the variables using Z-scores\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019354bd-fe23-494a-b090-4519a8d1fbf3",
   "metadata": {},
   "source": [
    "2. Neural Network architecture\n",
    "\n",
    "Since the network is not very deep, and the number of parameters is relatively small, I can employ more \"demanding\" (and performing) activation functions. In this case, I choose ELU (Exponential Linear Unit) activations. A softmax function is then applied at the end, so that the attribution of classes (M/B) is shrinked into probabilities.\n",
    "\n",
    "Additionally, I apply dropout in order to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72805450-2821-45d0-a8dd-41f409d98300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.activations import elu, softmax\n",
    "\n",
    "# Architecture\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden1 = 30\n",
    "n_hidden2 = 20\n",
    "n_hidden3 = 15\n",
    "n_output = y_train.shape[1]\n",
    "\n",
    "# set dropout probability\n",
    "dropout_prob = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a0732-a389-4260-9eb9-c42c06715cb4",
   "metadata": {},
   "source": [
    "The definition of a model in TensorFlow 2.0 follows the syntax of Keras' Sequential() models.\n",
    "\n",
    "Each layer is defined by the Dense() function, taking as inputs: the previous layer, the number of nodes, and the activation function (it can actually take a lor of additional arguments, but I'll not review them here). The input layer also requires a definition of the input data shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96419197-fa47-4aeb-947c-0f36ca7e2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(n_input,)),  # Explicit input layer\n",
    "\n",
    "    Dense(n_input, activation=elu),   # Input layer (no input_shape now)\n",
    "\n",
    "    Dense(n_hidden1, activation=elu),  # Hidden layer 1\n",
    "    Dropout(dropout_prob),\n",
    "\n",
    "    Dense(n_hidden2, activation=elu),  # Hidden layer 2\n",
    "    Dropout(dropout_prob),\n",
    "\n",
    "    Dense(n_hidden3, activation=elu),  # Hidden layer 3\n",
    "    Dropout(dropout_prob),\n",
    "\n",
    "    Dense(n_output, activation=softmax)  # Output layer\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eda25b81-99df-420a-b635-0c16ca1d5868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">      Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">620</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">315</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)            │          \u001b[38;5;34m930\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)            │          \u001b[38;5;34m930\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)            │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)            │          \u001b[38;5;34m620\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)            │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)            │          \u001b[38;5;34m315\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)            │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)             │           \u001b[38;5;34m32\u001b[0m │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,827</span> (11.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,827\u001b[0m (11.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,827</span> (11.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,827\u001b[0m (11.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a7e54-07fb-4903-ac91-94319169ee8f",
   "metadata": {},
   "source": [
    "3. Implementation of full-Batch Gradient Descent\n",
    "This is Gradient Descent in its simplest form, in which the whole bunch (erhm, batch) of training data is fed into the network at each iteration.\n",
    "\n",
    "(In a following Notebook I will show the implementation of a more powerful technique: Mini-Batch Gradient Descent.)\n",
    "\n",
    "In order to train the model, I neeed to define a loss function (that Gradient Descent will minimize), an accuracy metrics (in oreder to monitor the model's performance through the epochs) and an optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5eff2972-fb3d-4d3f-a371-e38578af1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss: Binary cross-entropy is made for \n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Binary Accuracy (expressed in the [0,1] interval)\n",
    "accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Adam Optimizer (what you'll need 99.99% of the time)\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1c068-a651-4cb3-92ca-c96121f90d6e",
   "metadata": {},
   "source": [
    "There are at least two ways to train a Neural Network in TensorFlow 2.0. The first is using Keras, by calling:\n",
    "\n",
    "model.compile(optimizer, loss, metrics)\n",
    "The other is to use pure TensorFlow's eager execution method, which is what I'll do here. In TensorFlow 1.x what you had to do was to create a computational graph (with placeholders instead of actual data), and then running it in a tf.Session(). Eager execution instead is the TensorFlow's variant of imperative programming: operations are evaluated immediately without building graphs, and run instantly. Eager execution makes code is easier to debug, and your whole script much less verbose and easier to read (now it looks pretty much like canonical Python). Another reason why Google's engineers changed TensorFlow so profoudly is that symbolic programming was not so popular (and they want TensorFlow to keep competing against pyTorch, which gained a lot in popularity recently).\n",
    "\n",
    "The reason why I employ eager execution is that I think it gives you more control on model training, and a better control of the output. Also, it forces you to better understand how training a Neural Network works.\n",
    "\n",
    "The fundamental element of training a Network in eager execution is represented by tf.GradientTape(). This object calculates and stores the gradient of the loss function at each iteration of the training operation. Once you generate a GradientTape, you can call the .gradient() argument to get the actual gradient (i.e. the first derivative of the loss function). Later, you feed this values into an optimizer using the .apply_gradients argument that updates the Network's trainable variables (the very act of \"learning\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5f4de89-ef86-400c-8c3f-b56b94818383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.\tTraining Loss: 0.3511873,\tAccuracy: 0.9882629\n",
      "200.\tTraining Loss: 0.22034082,\tAccuracy: 0.9882629\n",
      "300.\tTraining Loss: 0.20023172,\tAccuracy: 0.9882629\n",
      "400.\tTraining Loss: 0.19420436,\tAccuracy: 0.9882629\n",
      "500.\tTraining Loss: 0.19174807,\tAccuracy: 0.9882629\n",
      "600.\tTraining Loss: 0.19052464,\tAccuracy: 0.9882629\n",
      "700.\tTraining Loss: 0.18982877,\tAccuracy: 0.9882629\n",
      "800.\tTraining Loss: 0.18939582,\tAccuracy: 0.9882629\n",
      "900.\tTraining Loss: 0.1891083,\tAccuracy: 0.9882629\n",
      "1000.\tTraining Loss: 0.18890786,\tAccuracy: 0.9882629\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "###  TRAINING\n",
    "\n",
    "# save loss and accuracy improvements through epochs\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# iterate for 1000 epochs\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    # GratientTape is what I need in order to calculate gradients of the loss function\n",
    "    with tf.GradientTape() as tape:\n",
    "        # take current binary cross-entropy (bce_loss)\n",
    "        current_loss = bce_loss(model(X_train), y_train)\n",
    "    \n",
    "    # HERE THE ACTUAL TRAINING HAPPENS:\n",
    "    # Update weights based on the gradient of the loss function\n",
    "    gradients = tape.gradient(current_loss, model.trainable_variables)    # get the gradient of the loss function\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # update the weights\n",
    "    \n",
    "    # save current loss in its history vector\n",
    "    loss_history.append(current_loss.numpy())\n",
    "    \n",
    "    # save current accuracy in its history vector\n",
    "    accuracy.update_state(y_train, model(X_train))  # this computes the accuracy and stores it\n",
    "    current_accuracy = accuracy.result().numpy()  # save its result as numpy object\n",
    "    accuracy_history.append(current_accuracy)\n",
    "    \n",
    "    # In order to monitor progress, I will print loss and accuracy scores every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(str(epoch+1) + '.\\tTraining Loss: ' + str(current_loss.numpy()) + ',\\tAccuracy: ' + str(current_accuracy))\n",
    "    \n",
    "    accuracy.reset_state()  # reset the state of accuracy object for next iteration\n",
    "#\n",
    "print('\\nTraining complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2015fd-78de-45db-bafd-0b9eb36aac3b",
   "metadata": {},
   "source": [
    "4. Visualization\n",
    "   \n",
    "Once the training is done, let's check the model's improvement visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "540267f4-5de5-444c-b5c9-2bbb4ea6090a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using plt.subplot I can show paired loss and accuracy\n",
    "\n",
    "plt.figure(figsize = (15, 4))  # adjust figures size\n",
    "plt.subplots_adjust(wspace=0.2)  # adjust distance between plots\n",
    "\n",
    "# loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Binary Cross-Entropy')\n",
    "\n",
    "# accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_history)\n",
    "plt.title('Accuracy History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7aad15-a128-41c5-b3a2-909f5a2e2686",
   "metadata": {},
   "source": [
    "\n",
    "Accuracy scores reached .97+ levels after less than 200 epochs, which is good.\n",
    "\n",
    "At this point, we can make a prediction on the test set, and check the Network's ability to generalize beyond the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fc93d9b-bdf4-4f36-ad2e-bd6cf0427344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)    # (Keras syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c60322bb-44be-440b-97db-3714cc98bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs vector containing the value of the class with the highest predicted probability\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "\n",
    "# do it also for test data, i.e. reverse one-hot encoding and get a vector of 0-1 values\n",
    "testdata = np.argmax(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c51954a-10de-4009-8fed-384529109ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87  1]\n",
      " [ 2 53]]\n"
     ]
    }
   ],
   "source": [
    "# Now I can plot the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CM = confusion_matrix(prediction, testdata)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76775d22-04c6-4df3-a5fa-31654afde207",
   "metadata": {},
   "source": [
    "The Confusion Matrix looks very good: oservations on the matrix diagonal are correct predictions. Luckily, only three datapoints have been misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b1b68be-c386-430b-bd2e-cc28667431d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Accuracy = sum of the diagonal / sum of the whole matrix\n",
    "\n",
    "print('Test Accuracy: ' + str(np.sum(np.diag(CM)) / np.sum(CM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c22a00-195d-4b4d-87ba-d571ae0c1bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
